{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('...'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from instructor.environment import FindAllShapesEnv\n",
    "from instructor.callback import IoUCallback\n",
    "from datasets.shapes import generate_image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from datasets.shapes import Triangle\n",
    "\n",
    "for i in range(5):\n",
    "    entry = generate_image(8, create_mask=False, combinations=[(Triangle, c) for c in [\"red\", \"green\", \"blue\", \"purple\"]], scale=60)\n",
    "    entry.image.save(f\"triangles_{i}.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 57\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "set_random_seed(SEED)\n",
    "make_env = lambda: FindAllShapesEnv(lambda: generate_image(30, False, scale=1).shapes)\n",
    "vec_env = make_vec_env(make_env, n_envs=1024)\n",
    "model = DQN(\"MlpPolicy\", vec_env, verbose=0,\n",
    "            # gradient_steps=-1,\n",
    "            device=\"cpu\",\n",
    "            policy_kwargs={'net_arch': [64, 64]},\n",
    "            tensorboard_log='tb_test'\n",
    "            ).learn(450000, progress_bar=False, tb_log_name='sparse_ppo', callback=IoUCallback())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ious = []\n",
    "rewards = []\n",
    "env = make_env()\n",
    "set_random_seed(SEED)\n",
    "obs = env.reset()\n",
    "for _ in range(30000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        if 'iou' in info:\n",
    "            ious.append(info['iou'])\n",
    "print(f\"Mean IoU: {np.mean(ious)}\\nMean reward: {np.mean(rewards)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ious)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "env = FindAllShapesEnv(lambda: generate_image(8, False, scale=1).shapes)\n",
    "obs = env.reset()\n",
    "\n",
    "print(\", \".join(map(str, env.shapes)))\n",
    "\n",
    "n_steps = 70\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", env.action_dict[action.item()])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "    env.render(mode='console')\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
